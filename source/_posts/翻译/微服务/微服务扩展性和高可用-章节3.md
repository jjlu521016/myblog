---
title: 微服务扩展性和高可用-章节3
tags:
  - 翻译
  - 微服务
toc: true
originContent: ''
categories:
  - 翻译
  - 微服务
date: 2019-05-09 13:31:38
---

> 原文出处:https://dzone.com/refcardz/scalability?chapter=3
## Caching Strategies
Stateful load balancing techniques require data sharing among the service providers. Caching is a technique for sharing data among multiple consumers or servers that are expensive to either compute or fetch. Data are stored and retrieved in a subsystem that provides quick access to a copy of the frequently accessed data.

Caches are implemented as an indexed table where a unique key is used for referencing some datum. Consumers access data by checking (hitting) the cache first and retrieving the datum from it. If it's not there (cache miss), then the costlier retrieval operation takes place and the consumer or a subsystem inserts the datum to the cache.

### Write Policy
The cache may become stale if the backing store changes without updating the cache. A write policy for the cache defines how cached data are refreshed. Some common write policies include:

+ Write-through: Every write to the cache follows a synchronous write to the backing store.
+ Write-behind: Updated entries are marked in the cache table as dirty and it's updated only when a dirty datum is requested.
+ No-write allocation: Only read requests are cached under the assumption that the data won't change over time but it's expensive to retrieve.
### Application Caching
+ Implicit caching happens when there is little or no programmer participation in implementing the caching. The program executes queries and updates using its native API and the caching layer automatically caches the requests independently of the application. Example: Terracotta (https://www.terracotta.org/).

+ Explicit caching happens when the programmer participates in implementing the caching API and may also implement the caching policies. The program must import the caching API into its flow in order to use it. Examples: memcached (http://www.danga.com/memcached), Redis (https://redis.io), and Oracle Coherence (http://coherence.oracle.com).  

In general, implicit caching systems are specific to a platform or language. Terracotta, for example, only works with Java and JVM-hosted languages like Groovy or Kotlin. Explicit caching systems may be used with many programming languages and across multiple platforms at the same time. Memcached and Redis work with every major programming language, and Coherence works with Java, .Net, and native C++ applications.

### Web Caching
Web caching is used for storing documents or portions of documents (‘particles') to reduce server load, bandwidth usage and lag for web applications. Web caching can exist on the browser (user cache) or on the server, the topic of this section. Web caches are invisible to the client may be classified in any of these categories:

+ Web accelerators: they operate on behalf of the server of origin. Used for expediting access to heavy resources, like media files, and are often geolocated closer to intended recipients. Content distribution networks (CDNs) are an example of web acceleration caches; Akamai, Amazon S3, Nirvanix are examples of this technology.
+ Proxy caches: they serve requests to a group of clients that may all have access to the same resources. They can be used for content filtering and for reducing bandwidth usage. Squid, Apache, Amazon Cloud Front, ISA server are examples of this technology.  

### Distributed Caching
Caching techniques can be implemented across multiple systems that serve requests for multiple consumers and from multiple resources. These are known as distributed caches, like the setup in Figure 6. Akamai is an example of a distributed web cache, and memcached is an example of a distributed application cache.

![image.png](/images/2019/05/09/95ad3230-721b-11e9-b22a-7d284106ced1.png)

***Figure 6: Distributed Cache***